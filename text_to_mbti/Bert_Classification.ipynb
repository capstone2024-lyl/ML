{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "# Data Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "from numpy import loadtxt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Text Processing\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Model training and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Metrics\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, multilabel_confusion_matrix, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Ignore noise warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11928</th>\n",
       "      <td>INFP</td>\n",
       "      <td>I tire of winds and waters and pale lights! An...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11929</th>\n",
       "      <td>INFP</td>\n",
       "      <td>Guido, Guido, thou hast not spoke this hour, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11930</th>\n",
       "      <td>ISFJ</td>\n",
       "      <td>Yes, old Pia, good neighbor. Yes, Lisetta will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11931</th>\n",
       "      <td>INFP</td>\n",
       "      <td>Pia. Pia, turn my pillow, I am stifled. I have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11932</th>\n",
       "      <td>INFP</td>\n",
       "      <td>I have not eaten food this day. Hast thou Some...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       type                                              posts\n",
       "11928  INFP  I tire of winds and waters and pale lights! An...\n",
       "11929  INFP  Guido, Guido, thou hast not spoke this hour, N...\n",
       "11930  ISFJ  Yes, old Pia, good neighbor. Yes, Lisetta will...\n",
       "11931  INFP  Pia. Pia, turn my pillow, I am stifled. I have...\n",
       "11932  INFP  I have not eaten food this day. Hast thou Some..."
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading dataset\n",
    "data_set = pd.read_csv(\"./archive/combined_mbti_big_five.csv\")\n",
    "data_set.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type     False\n",
      "posts    False\n",
      "dtype: bool \n",
      "\n",
      "\n",
      "There are 11933 rows and 2 columns\n",
      "\n",
      "\n",
      "data types are  type     object\n",
      "posts    object\n",
      "dtype: object \n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11933 entries, 0 to 11932\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   type    11933 non-null  object\n",
      " 1   posts   11933 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 186.6+ KB\n",
      "None\n",
      "         type                                              posts\n",
      "count   11933                                              11933\n",
      "unique     16                                              11927\n",
      "top      INFP  I am just likely, am I not, to let that creatu...\n",
      "freq     2045                                                  2\n"
     ]
    }
   ],
   "source": [
    "# dataset info\n",
    "print(data_set.isnull().any(),\"\\n\\n\")\n",
    "nRow, nCol = data_set.shape\n",
    "print(f'There are {nRow} rows and {nCol} columns\\n\\n')\n",
    "print(\"data types are \", data_set.dtypes,\"\\n\\n\")\n",
    "print(data_set.info())\n",
    "print(data_set.describe(include=['object']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = np.unique(np.array(data_set['type']))\n",
    "total = data_set.groupby(['type']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>IE</th>\n",
       "      <th>NS</th>\n",
       "      <th>TF</th>\n",
       "      <th>JP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts  IE  NS  TF  JP\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...   1   1   0   1\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...   0   1   1   0\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...   1   1   1   0\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...   1   1   1   1\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce...   0   1   1   1"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 항목 별 columns 추가 \n",
    " \n",
    "def get_types(row):\n",
    "    t=row['type']\n",
    "\n",
    "    I = 0; N = 0\n",
    "    T = 0; J = 0\n",
    "    \n",
    "    if t[0] == 'I': I = 1\n",
    "    elif t[0] == 'E': I = 0\n",
    "    else: print('I-E not found') \n",
    "        \n",
    "    if t[1] == 'N': N = 1\n",
    "    elif t[1] == 'S': N = 0\n",
    "    else: print('N-S not found')\n",
    "        \n",
    "    if t[2] == 'T': T = 1\n",
    "    elif t[2] == 'F': T = 0\n",
    "    else: print('T-F not found')\n",
    "        \n",
    "    if t[3] == 'J': J = 1\n",
    "    elif t[3] == 'P': J = 0\n",
    "    else: print('J-P not found')\n",
    "    return pd.Series( {'IE':I, 'NS':N , 'TF': T, 'JP': J }) \n",
    "\n",
    "data = data_set.join(data_set.apply (lambda row: get_types (row),axis=1))\n",
    "data.head(5)\n",
    "#IE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introversion (I) /  Extroversion (E):\t 3095  /  8838\n",
      "Intuition (N) / Sensing (S):\t\t 3496  /  8437\n",
      "Thinking (T) / Feeling (F):\t\t 6720  /  5213\n",
      "Judging (J) / Perceiving (P):\t\t 7245  /  4688\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAGzCAYAAAAyiiOsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABFXUlEQVR4nO3deVgVdf//8ddhxwUQFZAkJTVTszQ1xdwlcSnTzLTIPa1uyczuFk0RLbMs1zT5tomV3pV9TU2NRLHwTjSXvN3StHBJA7pTwZ1tfn/0ZX6eQBwQPEd5Pq7rXHlm3vOZ95wZ4NWcOXNshmEYAgAAQJFcHN0AAADA9YDQBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AT8TUxMjGw22zVZV4cOHdShQwfz+bfffiubzaYvvvjimqx/8ODBql279jVZF24sfz92y2LMQ4cOyWazKS4urlTXA5QUoQk3tLi4ONlsNvPh5eWl4OBgRUREaM6cOTp9+nSprOf48eOKiYnRjh07SmW80uTMvZU3r732mpYtW+boNvB/9u7dq5iYGB06dMjRreA6QWhCuTB58mR9/PHHmj9/vp5++mlJ0ujRo9W4cWPt3LnTrnb8+PE6f/58scY/fvy4Jk2aVOxgsmbNGq1Zs6ZYyxRXUb2999572r9/f5muH//fjRSarsWxW6tWLZ0/f14DBgwok/H37t2rSZMmEZpgmZujGwCuhW7duql58+bm87FjxyoxMVH33XefevbsqZ9++kne3t6SJDc3N7m5le2Pxrlz51ShQgV5eHiU6XquxN3d3aHrx/XrWhy7+WeHrzf5P9+48XCmCeVWp06dNGHCBB0+fFiffPKJOb2wa5oSEhLUpk0b+fn5qVKlSqpfv77GjRsn6a/rkFq0aCFJGjJkiPlWYP51GB06dNDtt9+ubdu2qV27dqpQoYK57OWuC8nNzdW4ceMUFBSkihUrqmfPnjp69KhdTe3atTV48OACy1465pV6K+yaprNnz+q5555TSEiIPD09Vb9+fb311lsyDMOuzmazKSoqSsuWLdPtt98uT09PNWrUSPHx8YW/4JfIyspSdHS0mjVrJl9fX1WsWFFt27bV+vXrC9Tm5eVp9uzZaty4sby8vFS9enV17dpVW7dutav75JNPdPfdd6tChQqqUqWK2rVrV+BMyDvvvKNGjRrJ09NTwcHBGjlypE6dOmVXc+DAAfXp00dBQUHy8vJSzZo11b9/f2VkZJg1RR0Pl2Oz2XT27FktXLjQ3A+DBw/W+vXrZbPZ9OWXXxZYZvHixbLZbEpOTpb01/6qVKmSfv31V0VERKhixYoKDg7W5MmTC+yfvLw8zZo1S40aNZKXl5cCAwP1xBNP6OTJk3Z1W7duVUREhKpVqyZvb2+FhoZq6NChRW6LdPnr8T7//HNNmTJFNWvWlJeXlzp37qyDBw8WWP7dd99VnTp15O3trbvvvlsbNmwoUHO5a5r27dunhx9+WNWrV5e3t7fq16+vl19+2Zx/+PBh/eMf/1D9+vXl7e2tqlWrqm/fvnZnlOLi4tS3b19JUseOHc198u2335o1Vo6Xon6+S/rawnlxpgnl2oABAzRu3DitWbNGw4cPL7Rmz549uu+++3THHXdo8uTJ8vT01MGDB/X9999Lkho0aKDJkycrOjpaI0aMUNu2bSVJrVu3Nsf4888/1a1bN/Xv31+PPfaYAgMDi+xrypQpstlsevHFF5Wenq5Zs2YpPDxcO3bsMM+IWWGlt0sZhqGePXtq/fr1GjZsmJo0aaJvvvlGzz//vI4dO6aZM2fa1f/73//W0qVL9Y9//EOVK1fWnDlz1KdPHx05ckRVq1a9bF+ZmZl6//339cgjj2j48OE6ffq0PvjgA0VEROiHH35QkyZNzNphw4YpLi5O3bp10+OPP66cnBxt2LBBmzZtMs8eTpo0STExMWrdurUmT54sDw8Pbd68WYmJierSpYukv8LwpEmTFB4erqeeekr79+/X/PnztWXLFn3//fdyd3dXVlaWIiIidPHiRT399NMKCgrSsWPHtHLlSp06dUq+vr5XPB4u5+OPP9bjjz+uu+++WyNGjJAk1alTR61atVJISIgWLVqk3r172y2zaNEi1alTR2FhYea03Nxcde3aVa1atdK0adMUHx+viRMnKicnR5MnTzbrnnjiCcXFxWnIkCEaNWqUUlJSNHfuXP3444/m9qanp6tLly6qXr26XnrpJfn5+enQoUNaunRpkdtSlNdff10uLi765z//qYyMDE2bNk2RkZHavHmzWfPBBx/oiSeeUOvWrTV69Gj9+uuv6tmzp/z9/RUSElLk+Dt37lTbtm3l7u6uESNGqHbt2vrll1/01VdfacqUKZKkLVu2aOPGjerfv79q1qypQ4cOaf78+erQoYP27t2rChUqqF27dho1apTmzJmjcePGqUGDBpJk/tfK8ZKvsJ/vsnht4QQM4Aa2YMECQ5KxZcuWy9b4+voaTZs2NZ9PnDjRuPRHY+bMmYYk448//rjsGFu2bDEkGQsWLCgwr3379oYkIzY2ttB57du3N5+vX7/ekGTcdNNNRmZmpjn9888/NyQZs2fPNqfVqlXLGDRo0BXHLKq3QYMGGbVq1TKfL1u2zJBkvPrqq3Z1Dz30kGGz2YyDBw+a0yQZHh4edtP+85//GJKMt99+u8C6LpWTk2NcvHjRbtrJkyeNwMBAY+jQoea0xMREQ5IxatSoAmPk5eUZhmEYBw4cMFxcXIzevXsbubm5hdakp6cbHh4eRpcuXexq5s6da0gyPvzwQ8MwDOPHH380JBlLliy5bO9WjofLqVixYqH7bOzYsYanp6dx6tQpc1p6errh5uZmTJw40Zw2aNAgQ5Lx9NNP221jjx49DA8PD7OnDRs2GJKMRYsW2a0nPj7ebvqXX355xZ+Py7ncsdugQQO7fTt79mxDkrFr1y7DMAwjKyvLCAgIMJo0aWJX9+677xqS7MZMSUkpcOy2a9fOqFy5snH48GG7fvL3tWEYxrlz5wr0m5ycbEgyPvroI3PakiVLDEnG+vXr7WqtHi/5r0NhP99X89rCefH2HMq9SpUqFfkpOj8/P0nS8uXLlZeXV6J1eHp6asiQIZbrBw4cqMqVK5vPH3roIdWoUUOrV68u0fqtWr16tVxdXTVq1Ci76c8995wMw9DXX39tNz08PFx16tQxn99xxx3y8fHRr7/+WuR6XF1dzWti8vLydOLECeXk5Kh58+bavn27Wfe///u/stlsmjhxYoEx8t9CXbZsmfLy8hQdHS0XF5dCa9auXausrCyNHj3armb48OHy8fHRqlWrJEm+vr6SpG+++Ubnzp0rtPfSOB7+buDAgbp48aLdrSY+++wz5eTk6LHHHitQHxUVZf47/23SrKwsrV27VpK0ZMkS+fr66t5779V///tf89GsWTNVqlTJfBs0f1tWrlyp7OzsUtmWIUOG2F3vlH92M/+Y2Lp1q9LT0/Xkk0/a1Q0ePNh8/S/njz/+UFJSkoYOHaqbb77Zbt6lb6lfejY2Oztbf/75p+rWrSs/Pz+74+tyrB4v+Qr7+S6L1xaOR2hCuXfmzBm7gPJ3/fr10z333KPHH39cgYGB6t+/vz7//PNi/cG86aabinXhbL169eye22w21a1bt8w/5XP48GEFBwcXeD3y37I4fPiw3fS//+GSpCpVqhS4bqYwCxcu1B133CEvLy9VrVpV1atX16pVq+yuHfrll18UHBwsf3//y47zyy+/yMXFRQ0bNixyuySpfv36dtM9PDx0yy23mPNDQ0M1ZswYvf/++6pWrZoiIiI0b948u55K43j4u9tuu00tWrTQokWLzGmLFi1Sq1atVLduXbtaFxcX3XLLLXbTbr31Vkkyj48DBw4oIyNDAQEBql69ut3jzJkzSk9PlyS1b99effr00aRJk1StWjU98MADWrBggS5evFjibfn7MVGlShVJMo+J/Nf678e4u7t7ge36u/zgdfvttxdZd/78eUVHR5vX5VWrVk3Vq1fXqVOn7Pbl5Vg9XvIV9vNdFq8tHI/QhHLtt99+U0ZGRoE/TJfy9vZWUlKS1q5dqwEDBmjnzp3q16+f7r33XuXm5lpaT3GuQ7LqcjfgtNpTaXB1dS10uvG3i5L/7pNPPtHgwYNVp04dffDBB4qPj1dCQoI6depUamdvSmr69OnauXOnxo0bp/Pnz2vUqFFq1KiRfvvtN0mlczwUZuDAgfruu+/022+/6ZdfftGmTZsKPctkRV5engICApSQkFDoI//ap/wbqSYnJysqKkrHjh3T0KFD1axZM505c6ZE6y7pMVGann76aU2ZMkUPP/ywPv/8c61Zs0YJCQmqWrVqmRxfhf18l8VrC8cjNKFc+/jjjyVJERERRda5uLioc+fOmjFjhvbu3aspU6YoMTHRfJujtO8gfuDAAbvnhmHo4MGDdp90q1KlSoFP8kgFzwYVp7datWrp+PHjBd6u3Ldvnzm/NHzxxRe65ZZbtHTpUg0YMEAREREKDw/XhQsX7Orq1Kmj48eP68SJE5cdq06dOsrLy9PevXsvW5Pf99/vSZWVlaWUlJQC29W4cWONHz9eSUlJ2rBhg44dO6bY2Fhz/pWOh8spal/0799frq6u+te//qVFixbJ3d1d/fr1K1CXl5dX4O3Pn3/+WZLM46NOnTr6888/dc899yg8PLzA484777RbvlWrVpoyZYq2bt2qRYsWac+ePfr000+L3JaSyn+t/36MZ2dnKyUlpchl889E7d69u8i6L774QoMGDdL06dP10EMP6d5771WbNm0K/Lxcbn8U93gpyrV8bVH2CE0otxITE/XKK68oNDRUkZGRl60r7A92/qe78k+1V6xYUZIKDTEl8dFHH9kFly+++EK///67unXrZk6rU6eONm3apKysLHPaypUrC9yaoDi9de/eXbm5uZo7d67d9JkzZ8pms9mt/2rkn4249OzD5s2bzY/W5+vTp48Mw9CkSZMKjJG/bK9eveTi4qLJkycXOIuQXxMeHi4PDw/NmTPHbp0ffPCBMjIy1KNHD0l/faovJyfHbozGjRvLxcXF3NdWjofLqVix4mX3Q7Vq1dStWzd98sknWrRokbp27apq1aoVWnvp/jEMQ3PnzpW7u7s6d+4sSXr44YeVm5urV155pcCyOTk5Zg8nT54scAbI6raUVPPmzVW9enXFxsbaHbtxcXFXPEarV6+udu3a6cMPP9SRI0fs5l26Ha6urgW26+233y5wJvByPxtWj5eiOOK1RdnjlgMoF77++mvt27dPOTk5SktLU2JiohISElSrVi2tWLGiyBvoTZ48WUlJSerRo4dq1aql9PR0vfPOO6pZs6batGkj6a8A4+fnp9jYWFWuXFkVK1ZUy5YtFRoaWqJ+/f391aZNGw0ZMkRpaWmaNWuW6tata3dbhMcff1xffPGFunbtqocffli//PKLPvnkE7sLs4vb2/3336+OHTvq5Zdf1qFDh3TnnXdqzZo1Wr58uUaPHl1g7JK67777tHTpUvXu3Vs9evRQSkqKYmNj1bBhQ7u3Ljp27KgBAwZozpw5OnDggLp27aq8vDxt2LBBHTt2VFRUlOrWrauXX35Zr7zyitq2basHH3xQnp6e2rJli4KDgzV16lRVr15dY8eO1aRJk9S1a1f17NlT+/fv1zvvvKMWLVqYb4MlJiYqKipKffv21a233qqcnBx9/PHHcnV1VZ8+fSRZOx4up1mzZlq7dq1mzJih4OBghYaGqmXLlub8gQMH6qGHHpKkQgOPJHl5eSk+Pl6DBg1Sy5Yt9fXXX2vVqlUaN26cqlevLumv62meeOIJTZ06VTt27FCXLl3k7u6uAwcOaMmSJZo9e7YeeughLVy4UO+884569+6tOnXq6PTp03rvvffk4+Oj7t27l3wHF8Hd3V2vvvqqnnjiCXXq1En9+vVTSkqKFixYcMVrmiRpzpw5atOmje666y6NGDFCoaGhOnTokFatWmXe9f6+++7Txx9/LF9fXzVs2FDJyclau3ZtgdtgNGnSRK6urnrjjTeUkZEhT09PderUSQEBAZaOl6I44rXFNeCAT+wB10z+LQfyHx4eHkZQUJBx7733GrNnz7b7WH++v99yYN26dcYDDzxgBAcHGx4eHkZwcLDxyCOPGD///LPdcsuXLzcaNmxouLm52X1Mun379kajRo0K7e9yH9v+17/+ZYwdO9YICAgwvL29jR49ehT4iLVhGMb06dONm266yfD09DTuueceY+vWrQXGLKq3v99ywDAM4/Tp08azzz5rBAcHG+7u7ka9evWMN9980+4j3Ybx1y0HRo4cWaCny90K4VJ5eXnGa6+9ZtSqVcvw9PQ0mjZtaqxcubLQfnJycow333zTuO222wwPDw+jevXqRrdu3Yxt27bZ1X344YdG06ZNDU9PT6NKlSpG+/btjYSEBLuauXPnGrfddpvh7u5uBAYGGk899ZRx8uRJc/6vv/5qDB061KhTp47h5eVl+Pv7Gx07djTWrl1r1lg9Hgqzb98+o127doa3t7chqcDrdPHiRaNKlSqGr6+vcf78+QLLDxo0yKhYsaLxyy+/GF26dDEqVKhgBAYGGhMnTixwuwXD+Otj/M2aNTO8vb2NypUrG40bNzZeeOEF4/jx44ZhGMb27duNRx55xLj55psNT09PIyAgwLjvvvuMrVu3XnFbLnfs/v12DYXdNsAwDOOdd94xQkNDDU9PT6N58+ZGUlJSgTEvt+zu3buN3r17G35+foaXl5dRv359Y8KECeb8kydPGkOGDDGqVatmVKpUyYiIiDD27dtX6LH53nvvGbfccovh6upa4PYDVzpe8l+Hwn6+r+a1hfOyGcY1vDoPAHBZOTk5Cg4O1v33368PPvigwPzBgwfriy++4EJiwEG4pgkAnMSyZcv0xx9/aODAgY5uBUAhuKYJABxs8+bN2rlzp1555RU1bdpU7du3d3RLAArBmSYAcLD58+frqaeeUkBAgD766CNHtwPgMrimCQAAwALONAEAAFhAaAIAALCAC8FLSV5eno4fP67KlSuX+ldqAACAsmEYhk6fPq3g4GC5uBR9LonQVEqOHz+ukJAQR7cBAABK4OjRo6pZs2aRNYSmUlK5cmVJf73oPj4+Du4GAABYkZmZqZCQEPPveFEITaUk/y05Hx8fQhMAANcZK5fWcCE4AACABYQmAAAACwhNAAAAFnBNEwAAxWQYhnJycpSbm+voVnAFrq6ucnNzK5XbARGaAAAohqysLP3+++86d+6co1uBRRUqVFCNGjXk4eFxVeMQmgAAsCgvL08pKSlydXVVcHCwPDw8uKGxEzMMQ1lZWfrjjz+UkpKievXqXfEGlkUhNAEAYFFWVpby8vIUEhKiChUqOLodWODt7S13d3cdPnxYWVlZ8vLyKvFYXAgOAEAxXc3ZClx7pbW/HLrXk5KSdP/99ys4OFg2m03Lli0z52VnZ+vFF19U48aNVbFiRQUHB2vgwIE6fvy43RgnTpxQZGSkfHx85Ofnp2HDhunMmTN2NTt37lTbtm3l5eWlkJAQTZs2rUAvS5Ys0W233SYvLy81btxYq1evLpNtBgAA1yeHhqazZ8/qzjvv1Lx58wrMO3funLZv364JEyZo+/btWrp0qfbv36+ePXva1UVGRmrPnj1KSEjQypUrlZSUpBEjRpjzMzMz1aVLF9WqVUvbtm3Tm2++qZiYGL377rtmzcaNG/XII49o2LBh+vHHH9WrVy/16tVLu3fvLruNBwAA1xWbYRiGo5uQ/rp9+ZdffqlevXpdtmbLli26++67dfjwYd1888366aef1LBhQ23ZskXNmzeXJMXHx6t79+767bffFBwcrPnz5+vll19WamqqedX8Sy+9pGXLlmnfvn2SpH79+uns2bNauXKlua5WrVqpSZMmio2NtdR/ZmamfH19lZGRwdeoAMAN6sKFC0pJSVFoaGiBa2Nqv7TqmvVx6PUe12xdN4Ki9ltx/n5fV2/KZmRkyGazyc/PT5KUnJwsPz8/MzBJUnh4uFxcXLR582azpl27dnYfM4yIiND+/ft18uRJsyY8PNxuXREREUpOTr5sLxcvXlRmZqbdAwAAZzV48OAiT0wUpnbt2rLZbNq0aZPd9NGjR6tDhw6Wxjh06JBsNluRj7i4OEtjxcTEFLr82rVri7VdJXXdfHruwoULevHFF/XII4+YSTA1NVUBAQF2dW5ubvL391dqaqpZExoaalcTGBhozqtSpYpSU1PNaZfW5I9RmKlTp2rSpElXvV0AADgzLy8vvfjii/ruu+9KtHxISIh+//138/lbb72l+Ph4u6Dj6+trebxGjRoVCEn+/v4l6q24roszTdnZ2Xr44YdlGIbmz5/v6HYkSWPHjlVGRob5OHr0qKNbAgCg1I0YMUKbNm0q8QekXF1dFRQUZD4qVaokNzc3u2ne3t6Wx/v7skFBQVd900rL674ma7kK+YHp8OHDSkxMtHu/MSgoSOnp6Xb1OTk5OnHihIKCgsyatLQ0u5r851eqyZ9fGE9PT3l6epZ8w4orxnoKh0UxGY7uoCD2c+lztv3MPi4bzrafy9rxH4u/zLkT0oXTxVs2N0uh/u56ckAfjf3naHW9I/Cvj++fSZeyzpSsj9O/S9nnS7asgzn1mab8wHTgwAGtXbtWVatWtZsfFhamU6dOadu2bea0xMRE5eXlqWXLlmZNUlKSsrOzzZqEhATVr19fVapUMWvWrVtnN3ZCQoLCwsLKatMAALhujH/mcaUcPa5FSx1/O55du3apUqVK5uPuu+++Zut26JmmM2fO6ODBg+bzlJQU7dixQ/7+/qpRo4Yeeughbd++XStXrlRubq55jZG/v788PDzUoEEDde3aVcOHD1dsbKyys7MVFRWl/v37Kzg4WJL06KOPatKkSRo2bJhefPFF7d69W7Nnz9bMmTPN9T7zzDNq3769pk+frh49eujTTz/V1q1b7W5LAABAeVW9ahX988kBin4rVv16Rji0l/r162vFihXm82v5ro9DQ9PWrVvVsWNH8/mYMWMkSYMGDVJMTIz5ojRp0sRuufXr15tX7S9atEhRUVHq3LmzXFxc1KdPH82ZM8es9fX11Zo1azRy5Eg1a9ZM1apVU3R0tN29nFq3bq3Fixdr/PjxGjdunOrVq6dly5bp9ttvL6MtBwDg+jJmxGN6Z+ESvbPwc4f24eHhobp16zpk3Q4NTR06dFBRt4mycgspf39/LV68uMiaO+64Qxs2bCiypm/fvurbt+8V1wcAQHlUqWIFTRj9uGKm/496dmnv6HYcwqmvaQIAAM5jROSD8vWppMXL4h3dikM4/afnAAC4Htjdpfs6+GRY3GcrNGRMjIxj2y0v4+7urlee/4ceHTnObvq3G7eqY98RStm0UrVDgkvck+2mu7RgRowG9+t55WIHIDQBAFAOxM2yvyFzytFjah/WrMhlDm0u+NUwj/Tqqkd6dS0wVt3aIbopqPoV+4h57knFPPdkgekpR47Jzc1N97RocvllY2IUExNzxXWUFUITAADl0NfrN2ruqy+Wylir132v116Kkru7e8nHSPy3RkT2Vr1bbi6VnsoCoQkAgHLoh1Ufl9pYS96ddtVjjBzcrxQ6KVtcCA4AAGABoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWEJoAAAAsIDQBAABYwH2aAAAoDTG+125dI769duuCiTNNAACUA4NHT5TtprsKPLpGjizWOF9+nahW9w2U723tVPnWNmrU8SGNjn6zVHo5mHLE8hi1a9eWzWaTzWZTxYoVddddd2nJkiXF6qO4ONMEAEA50bVjay2YEWM3zdPDw/Ly6zZsVr+nXtKUF0eq573tZbPZtPfAr0pI2lQqvVSvWqVYY0yePFnDhw9XZmampk+frn79+ummm25S69ati92PFYQmAADKCU8PDwUFVCvx8l8lJOme5k30/FODzGm31qmlXl07XvNeJKly5coKCgpSUFCQ5s2bp08++URfffVVmYUm3p4DAACWBAVU056ff9HufQcd3UoBbm5ucnd3V1ZWVtmto8xGBgAATmXl2g2qVO8eu2njnh6qcaOGWVr+6aH9tWHzj2rc+WHVqllDre5qrC7tWymyd3d5elp/m6+wXrp1vKfEX/yblZWl6dOnKyMjQ506dSrRGFYQmgAAKCc6tm6u+VPH2k3z97P+qb+KFby16uM5+uXQUa3fuFWbtu/Sc5Nnavb7/1LyV3Gq4O1d4l4qVrC+bL4XX3xR48eP14ULF1SpUiW9/vrr6tGjR7HHsYrQBABAOVGxgrfqht581ePUqR2iOrVD9PijvfXyqGG6tW1vfbZijYb0e+Ca9vL8889r8ODBqlSpkgIDA2Wz2a5qvCshNAEAgBKrHRKsCt5eOnvu/DVfd7Vq1VS3bt1rtj5CEwAA5cTFrCylpv/Xbpqbm6uq+Vv7qH/M9FidO39B3Tu1Ua2aNXQq87TmfPAvZWfn6N62rcqiZadCaAIAoDTEZPz/fx//0XF9FCF+/UbVaNrFblr9OrW1L2mppL9uOnnot+P69ov3Cl2+fatmmhf3uQY+E620//6pKr4+anp7fa351zzVr1tbknTo6HGFtrpP65e8qw6tm5eoz7jPVmjImBgZx7aXaPmyQmgCAKAciJs1SXGzJhVZk3L0mDoWEXQ63tNCHe9pUfQYR47Jz7ey7mx4a5G9XKmP9mHNiqw5dOhQkfPLAqEJAAAoI/O0fjn0m1Z9NOeqxlmd+G+Ne3qoqvj5lHiMr9dv1NxXX7yqPsoCoQkAAMjXp7J+2xZ/1eO8OeHZqx7jh1UfX/UYZYE7ggMAAFhAaAIAALCA0AQAQDEZhuHoFlAMpbW/CE0AAFjk7u4uSTp37pyDO0Fx5O+v/P1XUlwIDgCARa6urvLz81N6erokqUKFCoV/dUcOZ6LKxIULxSo3DEPnzp1Tenq6/Pz85OrqelWrJzQBAFAMQUFBkmQGp0Kd+uMadVPOnE0p0WJ+fn7mfrsahCYAAIrBZrOpRo0aCggIUHZ2duFFc/te26bKi6itxV7E3d39qs8w5SM0AQBQAq6urpf/Y3zm6LVtprzw8nLo6rkQHAAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWEJoAAAAsIDQBAABY4NDQlJSUpPvvv1/BwcGy2WxatmyZ3XzDMBQdHa0aNWrI29tb4eHhOnDggF3NiRMnFBkZKR8fH/n5+WnYsGE6c+aMXc3OnTvVtm1beXl5KSQkRNOmTSvQy5IlS3TbbbfJy8tLjRs31urVq0t9ewEAwPXLoaHp7NmzuvPOOzVv3rxC50+bNk1z5sxRbGysNm/erIoVKyoiIkIXLlwwayIjI7Vnzx4lJCRo5cqVSkpK0ogRI8z5mZmZ6tKli2rVqqVt27bpzTffVExMjN59912zZuPGjXrkkUc0bNgw/fjjj+rVq5d69eql3bt3l93GAwCA64rNMAzD0U1Iks1m05dffqlevXpJ+ussU3BwsJ577jn985//lCRlZGQoMDBQcXFx6t+/v3766Sc1bNhQW7ZsUfPmzSVJ8fHx6t69u3777TcFBwdr/vz5evnll5WamioPDw9J0ksvvaRly5Zp3759kqR+/frp7NmzWrlypdlPq1at1KRJE8XGxlrqPzMzU76+vsrIyJCPj09pvSz/X4xv6Y9Z3sVkOLqDgtjPpc/Z9jP7uGywn8uHMtjPxfn77bTXNKWkpCg1NVXh4eHmNF9fX7Vs2VLJycmSpOTkZPn5+ZmBSZLCw8Pl4uKizZs3mzXt2rUzA5MkRUREaP/+/Tp58qRZc+l68mvy11OYixcvKjMz0+4BAABuXE4bmlJTUyVJgYGBdtMDAwPNeampqQoICLCb7+bmJn9/f7uawsa4dB2Xq8mfX5ipU6fK19fXfISEhBR3EwEAwHXEaUOTsxs7dqwyMjLMx9GjRx3dEgAAKENOG5qCgoIkSWlpaXbT09LSzHlBQUFKT0+3m5+Tk6MTJ07Y1RQ2xqXruFxN/vzCeHp6ysfHx+4BAABuXE4bmkJDQxUUFKR169aZ0zIzM7V582aFhYVJksLCwnTq1Clt27bNrElMTFReXp5atmxp1iQlJSk7O9usSUhIUP369VWlShWz5tL15NfkrwcAAMChoenMmTPasWOHduzYIemvi7937NihI0eOyGazafTo0Xr11Ve1YsUK7dq1SwMHDlRwcLD5CbsGDRqoa9euGj58uH744Qd9//33ioqKUv/+/RUcHCxJevTRR+Xh4aFhw4Zpz549+uyzzzR79myNGTPG7OOZZ55RfHy8pk+frn379ikmJkZbt25VVFTUtX5JAACAk3Jz5Mq3bt2qjh07ms/zg8ygQYMUFxenF154QWfPntWIESN06tQptWnTRvHx8fLy8jKXWbRokaKiotS5c2e5uLioT58+mjNnjjnf19dXa9as0ciRI9WsWTNVq1ZN0dHRdvdyat26tRYvXqzx48dr3LhxqlevnpYtW6bbb7/9GrwKAADgeuA092m63nGfpuuQs93XRWI/lwVn28/s47LBfi4fuE8TAACA8yM0AQAAWEBoAgAAsIDQBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWEJoAAAAsIDQBAABYQGgCAACwgNAEAABgAaEJAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALCA0AQAAWEBoAgAAsIDQBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWEJoAAAAsIDQBAABYQGgCAACwgNAEAABgAaEJAADAAkITAACABYQmAAAACwhNAAAAFjh1aMrNzdWECRMUGhoqb29v1alTR6+88ooMwzBrDMNQdHS0atSoIW9vb4WHh+vAgQN245w4cUKRkZHy8fGRn5+fhg0bpjNnztjV7Ny5U23btpWXl5dCQkI0bdq0a7KNAADg+uDUoemNN97Q/PnzNXfuXP3000964403NG3aNL399ttmzbRp0zRnzhzFxsZq8+bNqlixoiIiInThwgWzJjIyUnv27FFCQoJWrlyppKQkjRgxwpyfmZmpLl26qFatWtq2bZvefPNNxcTE6N13372m2wsAAJyXm6MbKMrGjRv1wAMPqEePHpKk2rVr61//+pd++OEHSX+dZZo1a5bGjx+vBx54QJL00UcfKTAwUMuWLVP//v31008/KT4+Xlu2bFHz5s0lSW+//ba6d++ut956S8HBwVq0aJGysrL04YcfysPDQ40aNdKOHTs0Y8YMu3AFAADKL6c+09S6dWutW7dOP//8syTpP//5j/7973+rW7dukqSUlBSlpqYqPDzcXMbX11ctW7ZUcnKyJCk5OVl+fn5mYJKk8PBwubi4aPPmzWZNu3bt5OHhYdZERERo//79OnnyZKG9Xbx4UZmZmXYPAABw43LqM00vvfSSMjMzddttt8nV1VW5ubmaMmWKIiMjJUmpqamSpMDAQLvlAgMDzXmpqakKCAiwm+/m5iZ/f3+7mtDQ0AJj5M+rUqVKgd6mTp2qSZMmlcJWAgCA64FTn2n6/PPPtWjRIi1evFjbt2/XwoUL9dZbb2nhwoWObk1jx45VRkaG+Th69KijWwIAAGXIqc80Pf/883rppZfUv39/SVLjxo11+PBhTZ06VYMGDVJQUJAkKS0tTTVq1DCXS0tLU5MmTSRJQUFBSk9Ptxs3JydHJ06cMJcPCgpSWlqaXU3+8/yav/P09JSnp+fVbyQAALguOPWZpnPnzsnFxb5FV1dX5eXlSZJCQ0MVFBSkdevWmfMzMzO1efNmhYWFSZLCwsJ06tQpbdu2zaxJTExUXl6eWrZsadYkJSUpOzvbrElISFD9+vULfWsOAACUP04dmu6//35NmTJFq1at0qFDh/Tll19qxowZ6t27tyTJZrNp9OjRevXVV7VixQrt2rVLAwcOVHBwsHr16iVJatCggbp27arhw4frhx9+0Pfff6+oqCj1799fwcHBkqRHH31UHh4eGjZsmPbs2aPPPvtMs2fP1pgxYxy16QAAwMk49dtzb7/9tiZMmKB//OMfSk9PV3BwsJ544glFR0ebNS+88ILOnj2rESNG6NSpU2rTpo3i4+Pl5eVl1ixatEhRUVHq3LmzXFxc1KdPH82ZM8ec7+vrqzVr1mjkyJFq1qyZqlWrpujoaG43AAAATDbj0ttro8QyMzPl6+urjIwM+fj4lP4KYnxLf8zyLibD0R0UxH4ufc62n9nHZYP9XD6UwX4uzt9vp357DgAAwFkQmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWEJoAAAAsIDQBAABYQGgCAACwgNAEAABgAaEJAADAAkITAACABYQmAAAAC9wc3QAAADea2hcWO7qFG9IhB6+fM00AAAAWEJoAAAAsIDQBAABYQGgCAACwgNAEAABgAaEJAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALChRaLrlllv0559/Fph+6tQp3XLLLVfdFAAAgLMpUWg6dOiQcnNzC0y/ePGijh07dtVNAQAAOJtifWHvihUrzH9/88038vX1NZ/n5uZq3bp1ql27dqk1BwAA4CyKFZp69eolSbLZbBo0aJDdPHd3d9WuXVvTp08vteYAAACcRbFCU15eniQpNDRUW7ZsUbVq1cqkKQC4UdW+sNjRLdyQDjm6AZQLxQpN+VJSUkq7DwAAAKdWotAkSevWrdO6deuUnp5unoHK9+GHH151YwAAAM6kRKFp0qRJmjx5spo3b64aNWrIZrOVdl8AAABOpUShKTY2VnFxcRowYEBp9wMAAOCUSnSfpqysLLVu3bq0ewEAAHBaJQpNjz/+uBYv5hMgAACg/CjR23MXLlzQu+++q7Vr1+qOO+6Qu7u73fwZM2aUSnMAAADOokShaefOnWrSpIkkaffu3XbzuCgcAADciEoUmtavX1/afQAAADi1El3TBAAAUN6U6ExTx44di3wbLjExscQNAQAAOKMShab865nyZWdna8eOHdq9e3eBL/IFAAC4EZQoNM2cObPQ6TExMTpz5sxVNQQAAOCMSvWapscee4zvnQMAADekUg1NycnJ8vLyKs0hAQAAnEKJ3p578MEH7Z4bhqHff/9dW7du1YQJE0qlMQAAAGdSojNNvr6+dg9/f3916NBBq1ev1sSJE0u1wWPHjumxxx5T1apV5e3trcaNG2vr1q3mfMMwFB0drRo1asjb21vh4eE6cOCA3RgnTpxQZGSkfHx85Ofnp2HDhhW49mrnzp1q27atvLy8FBISomnTppXqdgAAgOtbic40LViwoLT7KNTJkyd1zz33qGPHjvr6669VvXp1HThwQFWqVDFrpk2bpjlz5mjhwoUKDQ3VhAkTFBERob1795pvFUZGRur3339XQkKCsrOzNWTIEI0YMcL8/rzMzEx16dJF4eHhio2N1a5duzR06FD5+flpxIgR12RbAQCAcytRaMq3bds2/fTTT5KkRo0aqWnTpqXSVL433nhDISEhdiEtNDTU/LdhGJo1a5bGjx+vBx54QJL00UcfKTAwUMuWLVP//v31008/KT4+Xlu2bFHz5s0lSW+//ba6d++ut956S8HBwVq0aJGysrL04YcfysPDQ40aNdKOHTs0Y8YMQhMAAJBUwrfn0tPT1alTJ7Vo0UKjRo3SqFGj1KxZM3Xu3Fl//PFHqTW3YsUKNW/eXH379lVAQICaNm2q9957z5yfkpKi1NRUhYeHm9N8fX3VsmVLJScnS/rr4nQ/Pz8zMElSeHi4XFxctHnzZrOmXbt28vDwMGsiIiK0f/9+nTx5stDeLl68qMzMTLsHAAC4cZUoND399NM6ffq09uzZoxMnTujEiRPavXu3MjMzNWrUqFJr7tdff9X8+fNVr149ffPNN3rqqac0atQoLVy4UJKUmpoqSQoMDLRbLjAw0JyXmpqqgIAAu/lubm7y9/e3qylsjEvX8XdTp061u64rJCTkKrcWAAA4sxK9PRcfH6+1a9eqQYMG5rSGDRtq3rx56tKlS6k1l5eXp+bNm+u1116TJDVt2lS7d+9WbGysw+88PnbsWI0ZM8Z8npmZSXACAOAGVqIzTXl5eXJ3dy8w3d3dXXl5eVfdVL4aNWqoYcOGdtMaNGigI0eOSJKCgoIkSWlpaXY1aWlp5rygoCClp6fbzc/JydGJEyfsagob49J1/J2np6d8fHzsHgAA4MZVotDUqVMnPfPMMzp+/Lg57dixY3r22WfVuXPnUmvunnvu0f79++2m/fzzz6pVq5akvy4KDwoK0rp168z5mZmZ2rx5s8LCwiRJYWFhOnXqlLZt22bWJCYmKi8vTy1btjRrkpKSlJ2dbdYkJCSofv36dp/UAwAA5VeJQtPcuXOVmZmp2rVrq06dOqpTp45CQ0OVmZmpt99+u9Sae/bZZ7Vp0ya99tprOnjwoBYvXqx3331XI0eOlCTZbDaNHj1ar776qlasWKFdu3Zp4MCBCg4OVq9evST9dWaqa9euGj58uH744Qd9//33ioqKUv/+/RUcHCxJevTRR+Xh4aFhw4Zpz549+uyzzzR79my7t98AAED5VqJrmkJCQrR9+3atXbtW+/btk/RXOLn0U2yloUWLFvryyy81duxYTZ48WaGhoZo1a5YiIyPNmhdeeEFnz57ViBEjdOrUKbVp00bx8fF2X+eyaNEiRUVFqXPnznJxcVGfPn00Z84cc76vr6/WrFmjkSNHqlmzZqpWrZqio6O53QAAADDZDMMwrBYnJiYqKipKmzZtKnANT0ZGhlq3bq3Y2Fi1bdu21Bt1dpmZmfL19VVGRkbZXN8U41v6Y5Z3MRmO7qAg9nPpc7L9XPulVY5u4YZ06PUejm7BDvu5bJTFfi7O3+9ivT03a9YsDR8+vNBBfX199cQTT2jGjBnF6xYAAOA6UKzQ9J///Eddu3a97PwuXbrYXXANAABwoyhWaEpLSyv0VgP53NzcSvWO4AAAAM6iWKHppptu0u7duy87f+fOnapRo8ZVNwUAAOBsihWaunfvrgkTJujChQsF5p0/f14TJ07UfffdV2rNAQAAOIti3XJg/PjxWrp0qW699VZFRUWpfv36kqR9+/Zp3rx5ys3N1csvv1wmjQIAADhSsUJTYGCgNm7cqKeeekpjx45V/t0KbDabIiIiNG/evAJffAsAAHAjKPbNLWvVqqXVq1fr5MmTOnjwoAzDUL169fi6EQAAcEMr0R3BJalKlSpq0aJFafYCAADgtEr03XMAAADlDaEJAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALCA0AQAAWEBoAgAAsIDQBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWEJoAAAAsIDQBAABYQGgCAACwgNAEAABgAaEJAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALCA0AQAAWEBoAgAAsIDQBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhwXYWm119/XTabTaNHjzanXbhwQSNHjlTVqlVVqVIl9enTR2lpaXbLHTlyRD169FCFChUUEBCg559/Xjk5OXY13377re666y55enqqbt26iouLuwZbBAAArhfXTWjasmWL/ud//kd33HGH3fRnn31WX331lZYsWaLvvvtOx48f14MPPmjOz83NVY8ePZSVlaWNGzdq4cKFiouLU3R0tFmTkpKiHj16qGPHjtqxY4dGjx6txx9/XN9888012z4AAODcrovQdObMGUVGRuq9995TlSpVzOkZGRn64IMPNGPGDHXq1EnNmjXTggULtHHjRm3atEmStGbNGu3du1effPKJmjRpom7duumVV17RvHnzlJWVJUmKjY1VaGiopk+frgYNGigqKkoPPfSQZs6c6ZDtBQAAzue6CE0jR45Ujx49FB4ebjd927Ztys7Otpt+22236eabb1ZycrIkKTk5WY0bN1ZgYKBZExERoczMTO3Zs8es+fvYERER5hiFuXjxojIzM+0eAADgxuXm6Aau5NNPP9X27du1ZcuWAvNSU1Pl4eEhPz8/u+mBgYFKTU01ay4NTPnz8+cVVZOZmanz58/L29u7wLqnTp2qSZMmlXi7AADA9cWpzzQdPXpUzzzzjBYtWiQvLy9Ht2Nn7NixysjIMB9Hjx51dEsAAKAMOXVo2rZtm9LT03XXXXfJzc1Nbm5u+u677zRnzhy5ubkpMDBQWVlZOnXqlN1yaWlpCgoKkiQFBQUV+DRd/vMr1fj4+BR6lkmSPD095ePjY/cAAAA3LqcOTZ07d9auXbu0Y8cO89G8eXNFRkaa/3Z3d9e6devMZfbv368jR44oLCxMkhQWFqZdu3YpPT3drElISJCPj48aNmxo1lw6Rn5N/hgAAABOfU1T5cqVdfvtt9tNq1ixoqpWrWpOHzZsmMaMGSN/f3/5+Pjo6aefVlhYmFq1aiVJ6tKlixo2bKgBAwZo2rRpSk1N1fjx4zVy5Eh5enpKkp588knNnTtXL7zwgoYOHarExER9/vnnWrVq1bXdYAAA4LScOjRZMXPmTLm4uKhPnz66ePGiIiIi9M4775jzXV1dtXLlSj311FMKCwtTxYoVNWjQIE2ePNmsCQ0N1apVq/Tss89q9uzZqlmzpt5//31FREQ4YpMAAIATuu5C07fffmv33MvLS/PmzdO8efMuu0ytWrW0evXqIsft0KGDfvzxx9JoEQAA3ICc+pomAAAAZ0FoAgAAsIDQBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACy47m5uCdzIal9Y7OgWbjiHHN0AgBsGZ5oAAAAsIDQBAABYQGgCAACwgNAEAABgAaEJAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALCA0AQAAWEBoAgAAsIDvnrtO8J1kpe+QoxsAAFxXONMEAABgAaEJAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALCA0AQAAWEBoAgAAsIDQBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWEJoAAAAsIDQBAABYQGgCAACwgNAEAABgAaEJAADAAkITAACABU4dmqZOnaoWLVqocuXKCggIUK9evbR//367mgsXLmjkyJGqWrWqKlWqpD59+igtLc2u5siRI+rRo4cqVKiggIAAPf/888rJybGr+fbbb3XXXXfJ09NTdevWVVxcXFlvHgAAuI44dWj67rvvNHLkSG3atEkJCQnKzs5Wly5ddPbsWbPm2Wef1VdffaUlS5bou+++0/Hjx/Xggw+a83Nzc9WjRw9lZWVp48aNWrhwoeLi4hQdHW3WpKSkqEePHurYsaN27Nih0aNH6/HHH9c333xzTbcXAAA4LzdHN1CU+Ph4u+dxcXEKCAjQtm3b1K5dO2VkZOiDDz7Q4sWL1alTJ0nSggUL1KBBA23atEmtWrXSmjVrtHfvXq1du1aBgYFq0qSJXnnlFb344ouKiYmRh4eHYmNjFRoaqunTp0uSGjRooH//+9+aOXOmIiIirvl2AwAA5+PUZ5r+LiMjQ5Lk7+8vSdq2bZuys7MVHh5u1tx22226+eablZycLElKTk5W48aNFRgYaNZEREQoMzNTe/bsMWsuHSO/Jn+Mwly8eFGZmZl2DwAAcOO6bkJTXl6eRo8erXvuuUe33367JCk1NVUeHh7y8/Ozqw0MDFRqaqpZc2lgyp+fP6+omszMTJ0/f77QfqZOnSpfX1/zERISctXbCAAAnNd1E5pGjhyp3bt369NPP3V0K5KksWPHKiMjw3wcPXrU0S0BAIAy5NTXNOWLiorSypUrlZSUpJo1a5rTg4KClJWVpVOnTtmdbUpLS1NQUJBZ88MPP9iNl//puktr/v6Ju7S0NPn4+Mjb27vQnjw9PeXp6XnV2wYAAK4PTn2myTAMRUVF6csvv1RiYqJCQ0Pt5jdr1kzu7u5at26dOW3//v06cuSIwsLCJElhYWHatWuX0tPTzZqEhAT5+PioYcOGZs2lY+TX5I8BAADg1GeaRo4cqcWLF2v58uWqXLmyeQ2Sr6+vvL295evrq2HDhmnMmDHy9/eXj4+Pnn76aYWFhalVq1aSpC5duqhhw4YaMGCApk2bptTUVI0fP14jR440zxQ9+eSTmjt3rl544QUNHTpUiYmJ+vzzz7Vq1SqHbTsAAHAuTn2maf78+crIyFCHDh1Uo0YN8/HZZ5+ZNTNnztR9992nPn36qF27dgoKCtLSpUvN+a6urlq5cqVcXV0VFhamxx57TAMHDtTkyZPNmtDQUK1atUoJCQm68847NX36dL3//vvcbgAAAJic+kyTYRhXrPHy8tK8efM0b968y9bUqlVLq1evLnKcDh066Mcffyx2jwAAoHxw6jNNAAAAzoLQBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWEJoAAAAsIDQBAABYQGgCAACwgNAEAABgAaEJAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALCA0AQAAWEBoAgAAsIDQBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWEJoAAAAsIDQBAABYQGgCAACwgNAEAABgAaEJAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALCA0AQAAWEBo+pt58+apdu3a8vLyUsuWLfXDDz84uiUAAOAECE2X+OyzzzRmzBhNnDhR27dv15133qmIiAilp6c7ujUAAOBghKZLzJgxQ8OHD9eQIUPUsGFDxcbGqkKFCvrwww8d3RoAAHAwN0c34CyysrK0bds2jR071pzm4uKi8PBwJScnF6i/ePGiLl68aD7PyMiQJGVmZpZJf3kXz5XJuOVZWe2rq8F+Ln3Otp/Zx2WD/Vw+lMV+zh/TMIwr1hKa/s9///tf5ebmKjAw0G56YGCg9u3bV6B+6tSpmjRpUoHpISEhZdYjSpfvLEd3gGuB/Vw+sJ/Lh7Lcz6dPn5avr2+RNYSmEho7dqzGjBljPs/Ly9OJEydUtWpV2Ww2B3bmOJmZmQoJCdHRo0fl4+Pj6HZQRtjP5QP7uXxgP/91hun06dMKDg6+Yi2h6f9Uq1ZNrq6uSktLs5uelpamoKCgAvWenp7y9PS0m+bn51eWLV43fHx8yu0PX3nCfi4f2M/lQ3nfz1c6w5SPC8H/j4eHh5o1a6Z169aZ0/Ly8rRu3TqFhYU5sDMAAOAMONN0iTFjxmjQoEFq3ry57r77bs2aNUtnz57VkCFDHN0aAABwMELTJfr166c//vhD0dHRSk1NVZMmTRQfH1/g4nAUztPTUxMnTizwtiVuLOzn8oH9XD6wn4vHZlj5jB0AAEA5xzVNAAAAFhCaAAAALCA0AQAAWEBoAgAAsIDQBAAAYAGhCUUaPHiwevXqdcW677777rLfu/ftt9/KZrMV+khNTS3ljmHV4MGDZbPZ9Prrr9tNX7ZsmeWvAgoNDdXatWsLnfef//xHPXv2VEBAgLy8vFS7dm3169dP6enpV907rLvcz17+IyYmpsjlO3bsqPfff7/QeR06dCh0zJycnDLYElhl5fd2cX5nBwYGqk+fPvr111/LoNvrC6EJpWL58uW6//77i6zZv3+/fv/9d7tHQEDANeoQhfHy8tIbb7yhkydPFnvZnTt36uTJk2rfvn2BeX/88Yc6d+4sf39/ffPNN/rpp5+0YMECBQcH6+zZs6XROiy69Odt1qxZ8vHxsZv2z3/+87LLnjhxQt9//32RP9vDhw8v8HPt5sYtAJ2d1d/Zx48f15IlS7Rnzx7df//9ys3NvUYdOieObJSKFStWaO7cuUXWBAQE8P18TiY8PFwHDx7U1KlTNW3atGItu3z5cnXt2lXu7u4F5n3//ffKyMjQ+++/b/4BDQ0NVceOHUulb1h36Xdn+vr6ymazFfp9moVZtWqV7rrrriJv8FuhQgXL48F5FOd3do0aNRQdHa3IyEgdPHhQ9evXv0ZdOh/ONOGq7dmzR+np6erUqZOjW0Exubq66rXXXtPbb7+t3377rVjLrlixQg888ECh84KCgpSTk6Mvv/xS3D/3+lXUPsb1qyS/s729vSVJWVlZZdXWdYHQhKu2fPlyRUREyMPDo8i6mjVrqlKlSuajUaNG16hDFKV3795q0qSJJk6caHmZY8eOaefOnerWrVuh81u1aqVx48bp0UcfVbVq1dStWze9+eabSktLK622UcYuXryo+Ph49ezZs8i6d955x+7n+rnnnrtGHaKkrP7Ozvf777/rrbfe0k033VSuzzJJvD2HUrB8+XJFRUVdsW7Dhg2qXLmy+bywt3XgGG+88YY6depU5PUtl1qxYoXatGlT5NutU6ZM0ZgxY5SYmKjNmzcrNjZWr732mpKSktS4ceNS6hxlJTExUQEBAVf8n5vIyEi9/PLL5nPegnd+Vn9n16xZU4Zh6Ny5c7rzzjv1v//7v5aD1o2K0ISr8vvvv+vHH39Ujx49rlgbGhrKL1Qn1a5dO0VERGjs2LEaPHjwFetXrFhxxTMQklS1alX17dtXffv21WuvvaamTZvqrbfe0sKFC0uha5Qlq/vY19dXdevWvQYdoTQU53f2hg0b5OPjo4CAALv/4S3PCE24Kl999ZVat24tf39/R7eCq/T666+rSZMmVzz9fubMGa1fv17z588v1vgeHh6qU6cOn567DhiGoa+++kqffPKJo1tBKSvO72z+R7cgQhOuitX/G5Wk9PR0XbhwwW5a1apVeZvOSTRu3FiRkZGaM2dOkXXx8fG69dZbVbt27cvWrFy5Up9++qn69++vW2+91fwjvHr1ai1YsKCUO0dp27Ztm86dO6c2bdo4uhWUsuL8zkZBhCaU2NmzZ7Vu3TrNmjXLUn1hZzCSk5PVqlWrUu4MJTV58mR99tlnRdYsX778ir90GzZsqAoVKui5557T0aNH5enpqXr16un999/XgAEDSrNllIHly5ere/fu3G/pOpWXl1fovivu72wUZDP4PDBKaOnSpRo/frz27t3r6FZwjeTk5CgwMFBff/217r77bke3gzJyxx13aPz48Xr44Ycd3QpKoGvXrqpbt26B+zDxO/vqccsBlFilSpX0xhtvOLoNXEMnTpzQs88+qxYtWji6FZSRrKws9enT57K3k4DzOnnypFauXKlvv/1W4eHhBebzO/vqcaYJAIAbQO/evbVlyxYNGjRIr776quXvkIR1hCYAAAALeHsOAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALCA0AQAAWEBoAgAAsIDQBAAAYMH/A8l3P+KQ3C7cAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 항목 별 나온 횟수 \n",
    "print (\"Introversion (I) /  Extroversion (E):\\t\", data['IE'].value_counts()[0], \" / \", data['IE'].value_counts()[1])\n",
    "print (\"Intuition (N) / Sensing (S):\\t\\t\", data['NS'].value_counts()[0], \" / \", data['NS'].value_counts()[1])\n",
    "print (\"Thinking (T) / Feeling (F):\\t\\t\", data['TF'].value_counts()[0], \" / \", data['TF'].value_counts()[1])\n",
    "print (\"Judging (J) / Perceiving (P):\\t\\t\", data['JP'].value_counts()[0], \" / \", data['JP'].value_counts()[1])\n",
    "#Plotting the distribution of each personality type indicator\n",
    "N = 4\n",
    "bottom = (data['IE'].value_counts()[0], data['NS'].value_counts()[0], data['TF'].value_counts()[0], data['JP'].value_counts()[0])\n",
    "top = (data['IE'].value_counts()[1], data['NS'].value_counts()[1], data['TF'].value_counts()[1], data['JP'].value_counts()[1])\n",
    "\n",
    "ind = np.arange(N)    # the x locations for the groups\n",
    "# the width of the bars\n",
    "width = 0.7           # or len(x) can also be used here\n",
    "\n",
    "p1 = plt.bar(ind, bottom, width, label=\"I, N, T, F\")\n",
    "p2 = plt.bar(ind, top, width, bottom=bottom, label=\"E, S, F, P\") \n",
    "\n",
    "plt.title('Distribution accoss types indicators')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(ind, ('I / E',  'N / S', 'T / F', 'J / P',))\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binarize MBTI list: \n",
      "[[0 0 0 0]\n",
      " [1 0 1 1]\n",
      " [0 0 1 1]\n",
      " ...\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# 16가지 MBTI -> Binaryzation\n",
    "\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "# Stop word 제거\n",
    "useless_words = stopwords.words(\"english\")\n",
    "\n",
    "unique_type_list = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n",
    "       'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\n",
    "unique_type_list = [x.lower() for x in unique_type_list]\n",
    "\n",
    "# Splitting \n",
    "binary_Pers = {'I':0, 'E':1, 'N':0, 'S':1, 'F':0, 'T':1, 'J':0, 'P':1}\n",
    "binary_Pers_list = [{0:'I', 1:'E'}, {0:'N', 1:'S'}, {0:'F', 1:'T'}, {0:'J', 1:'P'}]\n",
    "\n",
    "def translate_personality(personality):\n",
    "    return [binary_Pers[l] for l in personality]\n",
    "\n",
    "# 결과 출력용\n",
    "def translate_back(personality):\n",
    "    # binary vector -> mbti personality\n",
    "    s = \"\"\n",
    "    for i, l in enumerate(personality):\n",
    "        s += binary_Pers_list[i][l]\n",
    "    return s\n",
    "\n",
    "list_personality_bin = np.array([translate_personality(p) for p in data.type])\n",
    "print(\"Binarize MBTI list: \\n%s\" % list_personality_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Post before preprocessing:\n",
      "\n",
      " 'http://www.youtube.com/watch?v=qsXHcwe3krw|||http://41.media.tumblr.com/tumblr_lfouy03PMA1qa1rooo1_500.jpg|||enfp and intj moments  https://www.youtube.com/watch?v=iz7lE1g4XM4  sportscenter not top ten plays  https://www.youtube.com/watch?v=uCdfze1etec  pranks|||What has been the most life-changing experience in your life?|||http://www.youtube.com/watch?v=vXZeYwwRDw8   http://www.youtube.com/watch?v=u8ejam5DP3E  On repeat for most of today.|||May the PerC Experience immerse you.|||The last thing my INFJ friend posted on his facebook before committing suicide the next day. Rest in peace~   http://vimeo.com/22842206|||Hello ENFJ7. Sorry to hear of your distress. It's only natural for a relationship to not be perfection all the time in every moment of existence. Try to figure the hard times as times of growth, as...|||84389  84390  http://wallpaperpassion.com/upload/23700/friendship-boy-and-girl-wallpaper.jpg  http://assets.dornob.com/wp-content/uploads/2010/04/round-home-design.jpg ...|||Welcome and stuff.|||http://playeressence.com/wp-content/uploads/2013/08/RED-red-the-pokemon-master-32560474-450-338.jpg  Game. Set. Match.|||Prozac, wellbrutin, at least thirty minutes of moving your legs (and I don't mean moving them while sitting in your same desk chair), weed in moderation (maybe try edibles as a healthier alternative...|||Basically come up with three items you've determined that each type (or whichever types you want to do) would more than likely use, given each types' cognitive functions and whatnot, when left by...|||All things in moderation.  Sims is indeed a video game, and a good one at that. Note: a good one at that is somewhat subjective in that I am not completely promoting the death of any given Sim...|||Dear ENFP:  What were your favorite video games growing up and what are your now, current favorite video games? :cool:|||https://www.youtube.com/watch?v=QyPqT8umzmY|||It appears to be too late. :sad:|||There's someone out there for everyone.|||Wait... I thought confidence was a good thing.|||I just cherish the time of solitude b/c i revel within my inner world more whereas most other time i'd be workin... just enjoy the me time while you can. Don't worry, people will always be around to...|||Yo entp ladies... if you're into a complimentary personality,well, hey.|||... when your main social outlet is xbox live conversations and even then you verbally fatigue quickly.|||http://www.youtube.com/watch?v=gDhy7rdfm14  I really dig the part from 1:46 to 2:50|||http://www.youtube.com/watch?v=msqXffgh7b8|||Banned because this thread requires it of me.|||Get high in backyard, roast and eat marshmellows in backyard while conversing over something intellectual, followed by massages and kisses.|||http://www.youtube.com/watch?v=Mw7eoU3BMbE|||http://www.youtube.com/watch?v=4V2uYORhQOk|||http://www.youtube.com/watch?v=SlVmgFQQ0TI|||Banned for too many b's in that sentence. How could you! Think of the B!|||Banned for watching movies in the corner with the dunces.|||Banned because Health class clearly taught you nothing about peer pressure.|||Banned for a whole host of reasons!|||http://www.youtube.com/watch?v=IRcrv41hgz4|||1) Two baby deer on left and right munching on a beetle in the middle.  2) Using their own blood, two cavemen diary today's latest happenings on their designated cave diary wall.  3) I see it as...|||a pokemon world  an infj society  everyone becomes an optimist|||49142|||http://www.youtube.com/watch?v=ZRCEq_JFeFM|||http://discovermagazine.com/2012/jul-aug/20-things-you-didnt-know-about-deserts/desert.jpg|||http://oyster.ignimgs.com/mediawiki/apis.ign.com/pokemon-silver-version/d/dd/Ditto.gif|||http://www.serebii.net/potw-dp/Scizor.jpg|||Not all artists are artists because they draw. It's the idea that counts in forming something of your own... like a signature.|||Welcome to the robot ranks, person who downed my self-esteem cuz I'm not an avid signature artist like herself. :proud:|||Banned for taking all the room under my bed. Ya gotta learn to share with the roaches.|||http://www.youtube.com/watch?v=w8IgImn57aQ|||Banned for being too much of a thundering, grumbling kind of storm... yep.|||Ahh... old high school music I haven't heard in ages.   http://www.youtube.com/watch?v=dcCRUPCdB1w|||I failed a public speaking class a few years ago and I've sort of learned what I could do better were I to be in that position again. A big part of my failure was just overloading myself with too...|||I like this person's mentality. He's a confirmed INTJ by the way. http://www.youtube.com/watch?v=hGKLI-GEc6M|||Move to the Denver area and start a new life for myself.'\n",
      "\n",
      "Post after preprocessing:\n",
      "\n",
      " 3350\n",
      "\n",
      "MBTI before preproc essing:\n",
      "\n",
      " INFJ\n",
      "\n",
      "MBTI after preprocessing:\n",
      "\n",
      " [0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Noise 제거\n",
    "def pre_process_text(data, remove_stop_words=True, remove_mbti_profiles=True):\n",
    "  list_personality = []\n",
    "  list_posts = []\n",
    "  len_data = len(data)\n",
    "  i=0\n",
    "  \n",
    "  for row in data.iterrows():\n",
    "      # check code working \n",
    "      # i+=1\n",
    "      # if (i % 500 == 0 or i == 1 or i == len_data):\n",
    "      #     print(\"%s of %s rows\" % (i, len_data))\n",
    "\n",
    "      #Remove and clean comments\n",
    "      posts = row[1].posts\n",
    "\n",
    "      #Remove url links \n",
    "      temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', posts)\n",
    "\n",
    "      temp = re.sub('\\|\\|\\|',' ',temp)\n",
    "\n",
    "      # transform mbti to binary vector\n",
    "      type_labelized = translate_personality(row[1].type) #or use lab_encoder.transform([row[1].type])[0]\n",
    "      list_personality.append(type_labelized)\n",
    "      # the cleaned data temp is passed here\n",
    "      list_posts.append(temp)\n",
    "\n",
    "  # returns the result\n",
    "  list_posts = np.array(list_posts)\n",
    "  list_personality = np.array(list_personality)\n",
    "  return list_posts, list_personality\n",
    "\n",
    "list_posts, list_personality  = pre_process_text(data, remove_stop_words=True, remove_mbti_profiles=True)\n",
    "\n",
    "print(\"\\nPost before preprocessing:\\n\\n\", data.posts[0])\n",
    "print(\"\\nPost after preprocessing:\\n\\n\", len(list_posts[0]))\n",
    "print(\"\\nMBTI before preproc essing:\\n\\n\", data.type[0])\n",
    "print(\"\\nMBTI after preprocessing:\\n\\n\", list_personality[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'    enfp and intj moments     sportscenter not top ten plays     pranks What has been the most life-changing experience in your life?        On repeat for most of today. May the PerC Experience immerse you. The last thing my INFJ friend posted on his facebook before committing suicide the next day. Rest in peace~     Hello ENFJ7. Sorry to hear of your distress. It's only natural for a relationship to not be perfection all the time in every moment of existence. Try to figure the hard times as times of growth, as... 84389  84390       ... Welcome and stuff.    Game. Set. Match. Prozac, wellbrutin, at least thirty minutes of moving your legs (and I don't mean moving them while sitting in your same desk chair), weed in moderation (maybe try edibles as a healthier alternative... Basically come up with three items you've determined that each type (or whichever types you want to do) would more than likely use, given each types' cognitive functions and whatnot, when left by... All things in moderation.  Sims is indeed a video game, and a good one at that. Note: a good one at that is somewhat subjective in that I am not completely promoting the death of any given Sim... Dear ENFP:  What were your favorite video games growing up and what are your now, current favorite video games? :cool:   It appears to be too late. :sad: There's someone out there for everyone. Wait... I thought confidence was a good thing. I just cherish the time of solitude b/c i revel within my inner world more whereas most other time i'd be workin... just enjoy the me time while you can. Don't worry, people will always be around to... Yo entp ladies... if you're into a complimentary personality,well, hey. ... when your main social outlet is xbox live conversations and even then you verbally fatigue quickly.    I really dig the part from 1:46 to 2:50   Banned because this thread requires it of me. Get high in backyard, roast and eat marshmellows in backyard while conversing over something intellectual, followed by massages and kisses.       Banned for too many b's in that sentence. How could you! Think of the B! Banned for watching movies in the corner with the dunces. Banned because Health class clearly taught you nothing about peer pressure. Banned for a whole host of reasons!   1) Two baby deer on left and right munching on a beetle in the middle.  2) Using their own blood, two cavemen diary today's latest happenings on their designated cave diary wall.  3) I see it as... a pokemon world  an infj society  everyone becomes an optimist 49142         Not all artists are artists because they draw. It's the idea that counts in forming something of your own... like a signature. Welcome to the robot ranks, person who downed my self-esteem cuz I'm not an avid signature artist like herself. :proud: Banned for taking all the room under my bed. Ya gotta learn to share with the roaches.   Banned for being too much of a thundering, grumbling kind of storm... yep. Ahh... old high school music I haven't heard in ages.     I failed a public speaking class a few years ago and I've sort of learned what I could do better were I to be in that position again. A big part of my failure was just overloading myself with too... I like this person's mentality. He's a confirmed INTJ by the way.   Move to the Denver area and start a new life for myself.'\"\n",
      " \"'I'm finding the lack of me in these posts very alarming. Sex can be boring if it's in the same position often. For example me and my girlfriend are currently in an environment where we have to creatively use cowgirl and missionary. There isn't enough... Giving new meaning to 'Game' theory. Hello *ENTP Grin*  That's all it takes. Than we converse and they do most of the flirting while I acknowledge their presence and return their words with smooth wordplay and more cheeky grins. This + Lack of Balance and Hand Eye Coordination. Real IQ test I score 127. Internet IQ tests are funny. I score 140s or higher.  Now, like the former responses of this thread I will mention that I don't believe in the IQ test. Before you banish... You know you're an ENTP when you vanish from a site for a year and a half, return, and find people are still commenting on your posts and liking your ideas/thoughts. You know you're an ENTP when you...     I over think things sometimes. I go by the old Sherlock Holmes quote.  Perhaps, when a man has special knowledge and special powers like my  own, it rather encourages him to seek a complex... cheshirewolf.tumblr.com  So is I :D 400,000+  post Not really; I've never thought of E/I or J/P as real functions.  I judge myself on what I use. I use Ne and Ti as my dominates. Fe for emotions and rarely Si. I also use Ni due to me strength... You know though. That was ingenious. After saying it I really want to try it and see what happens with me playing a first person shooter in the back while we drive around. I want to see the look on... out of all of them the rock paper one is the best. It makes me lol.  You guys are lucky :D I'm really high up on the tumblr system. So did you hear about that new first person shooter game? I've been rocking the hell out of the soundtrack on my auto sound equipment that will shake the heavens. We managed to put a couple PS3's in... No; The way he connected things was very Ne. Ne dominates are just as aware of their environments as Se dominates.  Example: Shawn Spencer or Patrick Jane; Both ENTPs. Well charlie I will be the first to admit I do get jealous like you do. I chalk it up to my 4w3 heart mixed with my dominate 7w8. 7s and 8s both like to be noticed. 4's like to be known (not the same... ;D I'll upload the same clip with the mic away from my mouth. Than you won't hear anything.  Ninja Assassin style but with splatter. Tik Tok is a really great song. As long as you can mental block out the singer. I love the beat it makes me bounce. drop.io v1swck0  :D Mic really close to my mouth and smokin aces: assassins ball playing in the background. Sociable =/= extrovert; I'm an extrovert and I'm not sociable. :) Sherlock in the movie was an ENTP. Normally he's played as a EXTJ. In the books he's an ESTJ.  As I said. The movie looked good except for it being called sherlock holmes.   Oh, I never had fear of kissing a guy. I will kiss an animal too. So there was nothing to vanish. Just personal taste and me not liking it.  The guy I kissed didn't know me. It was one of those... Sounds pretty much like my area and what I'm going through right now trying to figure out which way I want to take my life. I want to do so many things. The biggest problem is that I know if I don't... ;D I was operating under the impression that you were female. I never looked at your boxy. Okay, I help out my gay friends all the time and one of them has developed a little crush on me. I get red... T_T You just described me  and I'm living the worst nightmare. I'm trapped in one place with one one around. Only dull woods. If I was a serial killer this would be the perfect place but sadly I'm... TBH, and biased, sounds like a shadowed INFP. I think maybe he was hurt and turned ESTJ. I can tell because he has some of the typical INFP traits left over. *Checks list* I'm sorry. It seems that you have came at a bad time. We've already reached our quota of INFJs. However, being you're female and I like females I will make you a deal. I will kick one... I'm ANTP (Leaning toward E). I'm easy for both ENTPs and INTPs to identify with. :) I also imagine ENTP's interrogations would go a little bit like Jack's from 24 except more mechanical. Rigging up shock treatment equipment in an abandoned building out of an old car batty, jumper... It was a compliment :) Trust me. I'm just as psychopathic :D except I have emoticons. They're just weird ones. Like laughing when I get hurt or at people running themselves over with their lawn mower...     No. It's like a theme for where I live and that is why I know it by heart.     and I usual don't leave until the thing ends. But in the mean time. In between times. You work your thing. I'll work mine :D  ;D I'm the MBP; Pleasure to meet you. Damn, need to trust my instincts more I would have been closer I was going to say INFP. EXFP? Leaning toward S with the way she responded.  :D My friends, even my gay and lesbian ones, always come to me for advice. I bow to my entp masters ENTPs are so great. If it wasn't for ENTPs I wouldn't have been able to build what I'm building  Duck Duck  Duck  Shotgun What? Me? I never do that >.> <.< Because its hard to be sad about losing someone you like when you knew you were right and give yourself a big pat on the back because you're awesome and always correct. Oh, you don't have to tell me that most of them are stupid. I know this. That is why I play with them and it makes me laugh. :D As I'm going to take Neuropsychology and I have a few psychologist... :D I'm a Nightowl. I wake up between 6-7pm and stay awake till 10-11:30am. Personal opinion backed by theory would suggest that INTPs are the most socially difficult. While INTJs can be socially indifferent but they will also use social situations if the the need arises.... Personal stocks that I have on my desktop that I've downloaded from random stock sites and stock photobuckets. I'll tell you when I open photoshop.  :) Glad you like it static. :D Thanks.      Made for a friend. Several hours of work. I constructed every line by... :) Static:    I'll have to get to your avatar later if one of my fellow teammates doesn't. Psychologist don't keep me around long enough to diagnosis me. I like to toy with them. What I have diagnosis myself with and had a few psychologist friends (+ a few other friends) tell me I have is...'\"\n",
      " \"'Good one  _____     Of course, to which I say I know; that's my blessing and my curse. Does being absolutely positive that you and your best friend could be an amazing couple count? If so, than yes.   Or it's more I could be madly in love in case I reconciled my feelings (which at... No, I didn't; thank you for a link! So-called Ti-Si loop (and it can stem from any current topic/obsession) can be deadly. It's like when you're stuck in your own thoughts, and your mind just wanders in circles. Feels truly terrible. ... Have you noticed how peculiar vegetation can be? All you have to do is look down at the grass: dozens of different plant species there.    And now imagine that hundreds of years later (when/if soil... The Smiths – Never Had No One Ever I often find myself spotting faces on marble tiles/wood. This 5 year-old sentence is an incredibly accurate and beautiful description. I haven't visited this website in the last 3 years. So whoever reads this (and maybe even remembers me, which I highly doubt): hi.  700049  700057 When you sit in your garden until 10:30 PM writing songs, and sing them (together with dozens of crickets) while playing your acoustic guitar. This is the most INTP-ish thread I've ever seen. I wouldn't be able to look at the painting for the entire life if I knew that I picked it over the human being. I was drawing a background for my animation on which I'm working right now - it should have been Mars.. But I felt obligated to make Mark Watneyx92s postcard from it :D  If you read the book... I started to make comics about turtle Gordon and unicorn Chimes - here you can see two first stories:   INTJ Recently I started to post my comics about two friends - turtle Gordon and unicorn Chimes. Before that, I just posted stuff that interested me, but from now on I'll try to include only my works... Probably we could work together on a new model - I'm an expert in abrupt explosions of laughter upon various weird stuff. That happens because of peculiar sense of humor - so peculiar that not much... Hellooo Nah, you can touch it. Everyone thinks that it's scared or sad, but that's not true - in fact it has an absolutely neutral face. And this kitten actually really likes patting and hugs (only... Well.. kind of; As it was already mentioned, sometimes because of Ni it's hard to convey complex stuff which pops up in your head in whimsical compilations of shapes and pictures only with words.... I think this kitten would be very appropriate here.  376562 367034 GOOD NIGHT everyone out there! Even if for someone there is morning right now - nights always supersede mornings.. And people say good night in order to meet next day :) Oh, that movie :) It's awesome Thank you! Hope you had good sleep in the air; anyway, I'm wishing you good night for the next night ahead! (hopefully it will be on land)  Good people deserve good... 358882  358890 Well, other people who may be wondering about an issue from the name of the topic will find your response helpful anyway :) This. Finally someone mentioned that :) I still see creatures/faces in a maze of various random patterns. It can be amusing sometimes.  It's a very handy skill when you're bored. Oh, I didn't know that.. What a pity.  Why not sacrifice whole supermarket, then? We can decide which Walmart will be the best (I think the biggest one would be great). yippy  Here you go  357002  He thinks that the fire is delicious. Should I sacrifice tofu? I don't like to waste food. I don't think that the creator of this thread cares what's going on here after 3 years :) Heh, I understand you :) With these same given languages))) Yessss, Adventure Time :D I get angry quite rarely, but when I do, it's safer for surrounding people to go somewhere else. It's impossible for me to hide or suppress anger; the only way to get rid of this feeling is to burst... I've never liked it  Anything fake is bad, actually. Hugs should be given only to chosen ones. Chosen. There are quite few of them, though. 349890 Yup, you're doing it right :)   256818 Of course it's not very comfortable.  But. Human race survived thankfully women's ability to give birth to other human beings. It worked for thousands of years. Why change it? Besides, there are... That happens. And it occurs because most often people use results of extremely precise and elaborate online tests as a basis of determining one's type.      Both visual and language arts (more... 246386 I study graphic design now, which I really enjoy. What is interesting about this field, is that the ability to generate ideas and solve problems is much more important than possession of a specific... Alexxxandra97 - DeviantArt 236994   I am always ready to discipline (to intimidate, to be precise) my sibling's offender. World domination? Shooting people in the head? Why?  Oh, right, INTJs always must be characterised only with these words.     I want to show so badly my reaction to this: 221226 218106 ISTP?    \"\n",
      " ...\n",
      " 'Yes, old Pia, good neighbor. Yes, Lisetta will be well perhaps. God grant! \\'Tis not only of her I think, Pia, here am I Shut in this house from month to month a nurse; Here lies she sick, this child, and may not stir; And I, lacking due means to hire, must serve The house; while my best self, my soul, my art, Rust. My soul is scorched with holy thirst, My temples throb, my veins run fire; but yet, For all my dim distress and vague desire, No word, no single song, no verse, has come - O Blessed God! - stifled with creature needs, And with necessity about my throat! \\'Tis not that sun that maddens me, O Pia! Can you not see me shrunk? Have you not heard That other Guido of Perugia How he is grown? How lately at the feast That Ugolino, the great cardinal, Spread at Assisi Easter night, Guido Read certain of his verses and declaimed Pages of cursed sonnets to the guests. Yea. And when he ended, came the Duke Down from the dais to kiss that Guido\\'s hand Humbly, and said that poesy was king. And I, O God, I might have honor too Could I but break this prison where I drudge! Aye, yes, \\'tis true, she loveth me, she loveth me, And I love her. \\'Tis worse - add grief to care, And Poesy fares worse. Look, Pia, how she lieth there like death, That far - off patience on her face. Now, now, Surely I needs must make a song! And yet I may not; ashes and floor - sweeping clog My soul within me! Thou art most kind, good neighbor, to come here Helping our house. And it is very strange That when we are so kind we cannot know The heart also. For in my soul I hear bell summoning me always - For thy world is not my world, kind old friend. Then I will go, Pia. But not for long, I will come back soon enough to my chores, be sure; Mine is a short tether. Asleep, Lisetta? Lisetta, my love, I have been long from thee. So little I may do. My love, I have been long from thee, but now I will not leave thee any more. Oh, God, Let these kisses tell my heart to her. Along the stream I went and where it crossed Bevagna road - where the chestnut grows, thou knowest - Lisetta, I saw him. The brother, Francis of Assisi. Aye, him. There had he stopped to rest, being spent; And round him came the birds, beating their wings Upon his cloak and lighting on his arm. I saw him smile on them and heard him speak! \"My brother birds, little brothers, ye should love God Who gave you your wings and your bright songs and spread The soft air for you.\" He stroked their necks And blessed them. And then I saw his eyes. \"Father,\" I cried, \"speak thou to me, I faint Beside my way!\" \"Thou art as one that lieth at the gate Of Paradise and entereth not. For God Hath given thee thy soul for its own life, And not for glory among men.\" Lisetta, from his kind eyes I drank, and knew How God had magnified my soul through him, And sent me peace. And I returned to thee; For here in thee have I my glory. Thou art my saint and shrine! I pray thee rest thee now and sleep. Good - night. My full heart breaks in song; and I will sit Hearing the blessed saints within my soul, And will not stir from thee lest thou shouldst wake When I might not be near to serve thy need.'\n",
      " \"Pia. Pia, turn my pillow, I am stifled. I have not slept. Pia, think you I did not know? This month I scarce have slept for thinking on his lot. I read his fighting soul. Where are his songs, The great renown that waited him? Down, down, Struck by the self - same hand that shattered me. I listen night on night and hear him moan In his sleep - The padre from the village hemmed and said That God had sent me and my sickness here For Guido's cross to bear, his scourge. They thought I slept - Yea, loveth me somewhat but glory more. And I would have it so. O Mother of God, When wilt thou send me death? O Blessed Mother, I have lain so still! Death is the sister of all them that weep, Pia. For thy sake will I try. What, good Pia? The tears of women even in dreams may fall, Good brother. Wilt thou not bide? Aye, aye, the world lies open to thy hand, But unto me this twelvemonth is a death. The flesh is dead, and dying lies my soul, Shrunk like a flower in my fevered hand. I may not see the stars rise on the hills, Nor tend the flocks at even, nor rise to do Aught of the small sweet round of duties owed To him I love; but lie a burden to him, Calling on death who heareth not. Surely thy life is peace. Little Brother! Pia, the spring smiles on the tender grass, Surely the sun is brighter where he stood. Pia, 'twill be the gentlest of all eves. Surely God sent the brother for my need, To give His peace. Who is he, think you? Pia, thou art a good woman. I would not grieve thee. Pia, 'twas my love That sees thy goodness better than thyself. Pia. Pia, come near me here. Can you not see How much I love? If I could only speak To him or he to me, Guido, my love! His hand is near, but not his heart. Aye, tell me not. On winter nights I lay Hearing the tree limbs rattle there like hail, And from the corner eaves the dropping rain Like big dogs lapping all about - and he Spoke not to me. He sat beside his taper But never a line wrote down. Once I had words, Bright dreams, that shone through him, the same fire shone Through both, his songs were mine! But more his, Pia, more his! Not now, good Pia, 'tis not for food I die. 'Tis not for food. Wilt thou not read one song of these to me? Read on - and thou hast told me day by day Thou couldst not read. Fie! Give them to me here. O music locked amid the stones, My love hath spoken like to thee, Pia, think you - Pia, do you not hear The mowers and the reapers in the fields Singing the evening song, and the twilight pipes? The twilight is the hour when hearts break! How many lonely twilights will there be Ere God will spare me? Pia, good - night once more. Guido! Ah, I have need of naught, Guido. Thou needst not leave yet the pleasant air. Let not that trouble thee, my needs are few, And Pia is most kind. Thou hast already served to weariness. Guido, my love, perhaps I dream of thee! Perhaps God sends a dream to solace me. Yes, yes, I know, whom sawest thou? Guido, sawest thou him? Aye, and he said? Guido, what said he? Guido! Guido, the old spring comes back again. And now I may speak. Guido, look through my window vines there Where the stars rise. O Love, I have not slept For lacking thee. And often have I seen The moonlight lie like sleep upon the hill, And in the garden of the sky the moon Drift like a blown rose, Guido, and yet I might not speak. Now shall my dream become thy song again, And the long twilight be more sweet, Guido!\"\n",
      " \"I have not eaten food this day. Hast thou Somewhat that I may eat? I will eat then and bless thee. It is enough. He that hath eaten long The bread of the heart hath little hunger in him. Nay, I must go on. My daughter, child, Thou sleepest not for all thy lowered lids. Tears quiver on thy lashes, hast thou pain? I must fare on. My dear. My life hath given me words for thee to hear. There is a life larger than life, that dwells Invisible from all; whose lack alone Is death. There in thy soul the stars may rise, And at the even the gentle thoughts return To flock the quiet pastures of the mind; And in the large heart love is all thou owest For service unto God and thy Beloved. May you have God's peace, dear friends. Farewell.\"]\n"
     ]
    }
   ],
   "source": [
    "print(list_posts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IE: Introversion (I) / Extroversion (E)\n",
      "NS: Intuition (N) / Sensing (S)\n",
      "FT: Feeling (F) / Thinking (T)\n",
      "JP: Judging (J) / Perceiving (P)\n"
     ]
    }
   ],
   "source": [
    "personality_types = [ \"IE: Introversion (I) / Extroversion (E)\", \"NS: Intuition (N) / Sensing (S)\", \n",
    "                   \"FT: Feeling (F) / Thinking (T)\", \"JP: Judging (J) / Perceiving (P)\"  ]\n",
    "\n",
    "for personality_type in personality_types:\n",
    "    print(personality_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 텍스트 인코딩 함수\n",
    "def encode_texts(texts, tokenizer, max_length=512):\n",
    "    encoded = tokenizer(\n",
    "        list(texts),\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encoded['input_ids'], encoded['attention_mask']\n",
    "\n",
    "# 커스텀 데이터셋 클래스\n",
    "class PersonalityDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'label': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for personality type IE: Introversion (I) / Extroversion (E)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m Y_type \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([label[i] \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m list_personality], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 텍스트 데이터를 인코딩\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m X_input_ids, X_attention_masks \u001b[38;5;241m=\u001b[39m \u001b[43mencode_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_posts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 인코딩된 데이터셋 생성\u001b[39;00m\n\u001b[0;32m     11\u001b[0m dataset \u001b[38;5;241m=\u001b[39m PersonalityDataset(X_input_ids, X_attention_masks, Y_type)\n",
      "Cell \u001b[1;32mIn[92], line 5\u001b[0m, in \u001b[0;36mencode_texts\u001b[1;34m(texts, tokenizer, max_length)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_texts\u001b[39m(texts, tokenizer, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m     encoded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], encoded[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\HaeGang\\anaconda3\\envs\\ml\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2530\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2528\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2529\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2530\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[0;32m   2531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2532\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\HaeGang\\anaconda3\\envs\\ml\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2616\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2613\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2614\u001b[0m         )\n\u001b[0;32m   2615\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[1;32m-> 2616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2617\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2618\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2619\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   2620\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   2621\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   2622\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   2623\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2624\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2625\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2626\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2627\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2628\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2629\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2630\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2631\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   2632\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   2633\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2634\u001b[0m     )\n\u001b[0;32m   2635\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2636\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m   2637\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   2638\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2654\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2655\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\HaeGang\\anaconda3\\envs\\ml\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2807\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2797\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2798\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2799\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   2800\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2804\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2805\u001b[0m )\n\u001b[1;32m-> 2807\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m   2808\u001b[0m     batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2809\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2810\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[0;32m   2811\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   2812\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   2813\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   2814\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2815\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2816\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2817\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2818\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2819\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2820\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2821\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2822\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   2823\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   2824\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2825\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\HaeGang\\anaconda3\\envs\\ml\\lib\\site-packages\\transformers\\tokenization_utils.py:733\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     ids, pair_ids \u001b[38;5;241m=\u001b[39m ids_or_pair_ids\n\u001b[1;32m--> 733\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(pair_ids) \u001b[38;5;28;01mif\u001b[39;00m pair_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    735\u001b[0m input_ids\u001b[38;5;241m.\u001b[39mappend((first_ids, second_ids))\n",
      "File \u001b[1;32mc:\\Users\\HaeGang\\anaconda3\\envs\\ml\\lib\\site-packages\\transformers\\tokenization_utils.py:700\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus.<locals>.get_input_ids\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 700\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    701\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[0;32m    702\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\HaeGang\\anaconda3\\envs\\ml\\lib\\site-packages\\transformers\\tokenization_utils.py:547\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[0;32m    546\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 547\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    548\u001b[0m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "File \u001b[1;32mc:\\Users\\HaeGang\\anaconda3\\envs\\ml\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:244\u001b[0m, in \u001b[0;36mBertTokenizer._tokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    242\u001b[0m split_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_basic_tokenize:\n\u001b[1;32m--> 244\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasic_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnever_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_special_tokens\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;66;03m# If the token is part of the never_split set\u001b[39;00m\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasic_tokenizer\u001b[38;5;241m.\u001b[39mnever_split:\n\u001b[0;32m    247\u001b[0m             split_tokens\u001b[38;5;241m.\u001b[39mappend(token)\n",
      "File \u001b[1;32mc:\\Users\\HaeGang\\anaconda3\\envs\\ml\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:427\u001b[0m, in \u001b[0;36mBasicTokenizer.tokenize\u001b[1;34m(self, text, never_split)\u001b[0m\n\u001b[0;32m    425\u001b[0m     token \u001b[38;5;241m=\u001b[39m token\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrip_accents \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_strip_accents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrip_accents:\n\u001b[0;32m    429\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_strip_accents(token)\n",
      "File \u001b[1;32mc:\\Users\\HaeGang\\anaconda3\\envs\\ml\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:443\u001b[0m, in \u001b[0;36mBasicTokenizer._run_strip_accents\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cat \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    442\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 443\u001b[0m     \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(output)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, personality_type in enumerate(personality_types):\n",
    "    print(f\"Training model for personality type {personality_type}...\")\n",
    "\n",
    "    # 현재 유형에 대한 타깃 데이터 준비\n",
    "    Y_type = torch.tensor([label[i] for label in list_personality], dtype=torch.long)\n",
    "\n",
    "    # 텍스트 데이터를 인코딩\n",
    "    X_input_ids, X_attention_masks = encode_texts(list_posts, tokenizer)\n",
    "\n",
    "    # 인코딩된 데이터셋 생성\n",
    "    dataset = PersonalityDataset(X_input_ids, X_attention_masks, Y_type)\n",
    "\n",
    "    # 데이터 분할: 학습 70%, 검증 15%, 테스트 15%\n",
    "    train_size = int(0.7 * len(dataset))\n",
    "    val_size = int(0.15 * len(dataset))\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    # DataLoader 생성\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "    config.hidden_dropout_prob = 0.3\n",
    "    config.attention_probs_dropout_prob = 0.3\n",
    "    config.num_labels = 2\n",
    "\n",
    "    # BERT 모델 로드\n",
    "    bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config = config)\n",
    "    bert_model.to(device)\n",
    "\n",
    "    # 옵티마이저와 손실 함수 설정\n",
    "    optimizer = AdamW(bert_model.parameters(), lr=2e-5)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 모델 학습 및 검증\n",
    "    num_epochs = 7\n",
    "    for epoch in range(num_epochs):\n",
    "        bert_model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct += (outputs.logits.argmax(1) == labels).sum().item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        accuracy = total_correct / len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} - Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # 검증 과정\n",
    "        bert_model.eval()\n",
    "        val_loss, val_correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "                loss = loss_fn(outputs.logits, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_correct += (outputs.logits.argmax(1) == labels).sum().item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = val_correct / len(val_loader.dataset)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} - Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    " \n",
    "    # 모델 저장\n",
    "    bert_model.save_pretrained(f'./bert_model_{personality_type[:2]}')\n",
    "    print(f\"Model training and saving for {personality_type} completed.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Testing model for personality type IE...\n",
      "Probabilities for IE: [0.6633582  0.33664173]\n",
      "2\n",
      "\n",
      "Testing model for personality type NS...\n",
      "Probabilities for NS: [0.7296825  0.27031755]\n",
      "2\n",
      "\n",
      "Testing model for personality type FT...\n",
      "Probabilities for FT: [0.2642275  0.73577255]\n",
      "2\n",
      "\n",
      "Testing model for personality type JP...\n",
      "Probabilities for JP: [0.06783866 0.9321614 ]\n",
      "2\n",
      "{'energy': 33, 'recognition': 27, 'decision': 73, 'lifeStyle': 93}\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "def process_text(word):\n",
    "    word = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', word)\n",
    "    word = re.sub('\\|\\|\\|',' ',word)\n",
    "    return word    \n",
    "\n",
    "\n",
    "def read_notepad_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# 메모장 파일 경로\n",
    "notepad_file_path = './test_code.txt'\n",
    "\n",
    "# 메모장 파일 읽기\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "original_text = read_notepad_file(notepad_file_path)\n",
    "processed_text = process_text(original_text)\n",
    "\n",
    "\n",
    "# Encode the texts\n",
    "X_input_ids, X_attention_masks = encode_texts(processed_text, tokenizer)\n",
    "\n",
    "# GPU로 데이터 이동\n",
    "X_input_ids = X_input_ids.to(device)\n",
    "X_attention_masks = X_attention_masks.to(device)\n",
    "\n",
    "personality_types = ['IE', 'NS', 'FT', 'JP']\n",
    "probabilities = {\n",
    "    \"energy\": 0,\n",
    "    \"recognition\": 0,\n",
    "    \"decision\": 0,\n",
    "    \"lifeStyle\": 0\n",
    "}\n",
    "\n",
    "for personality_type in personality_types:\n",
    "    print(f\"\\nTesting model for personality type {personality_type}...\")\n",
    "    \n",
    "    # 모델 로드\n",
    "    bert_model = BertForSequenceClassification.from_pretrained(f'./bert_model_{personality_type}')\n",
    "    bert_model.to(device)\n",
    "    bert_model.eval()\n",
    "\n",
    "    # 예측 수행\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(X_input_ids, attention_mask=X_attention_masks)\n",
    "        probs = F.softmax(outputs.logits, dim=1)\n",
    "        predicted_probs = probs.cpu().numpy().flatten()\n",
    "        \n",
    "    print(f\"Probabilities for {personality_type}: {predicted_probs}\")\n",
    "    print(len(predicted_probs))\n",
    "    if personality_type == 'IE':\n",
    "        probabilities[\"energy\"] = int(predicted_probs[1] * 100)  # E의 확률\n",
    "    elif personality_type == 'NS':\n",
    "        probabilities[\"recognition\"] = int(predicted_probs[1] * 100)  # S의 확률\n",
    "    elif personality_type == 'FT':\n",
    "        probabilities[\"decision\"] = int(predicted_probs[1] * 100)  # T의 확률\n",
    "    elif personality_type == 'JP':\n",
    "        probabilities[\"lifeStyle\"] = int(predicted_probs[1] * 100)  # P의 확률\n",
    "\n",
    "print(probabilities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
